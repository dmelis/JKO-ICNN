import numpy as np
import torch
import torch.nn as nn
import pdb
from geomloss import SamplesLoss

from cpflow.lib.logdet_estimators import stochastic_logdet_gradient_estimator, unbiased_logdet, stochastic_lanczos_quadrature
from .utils import sample_rademacher, meshgrid_from

OVERRIDE_NANS_WARNING = True


class Functional(nn.Module):
    """ Parent class for functionals over measures.

        Attributes:
            _requires_x (bool): whether F requires input x for its computation
            _requires_u (bool): whether F requires function u for its computation
            _requires_g (bool): whether F requires gradient g for its computation
            _requires_hvp (bool): whether F requires hessian-vector product callable
                for its computation
            _requires_H (bool): whether F requires full hessian H for its computation

        Methods:
            header_str: produces formatted string for optimization log header
            update_str: produces formatted string for optimization log iterates

    """
    def __init__(self, *args, **kwargs):
        super(Functional, self).__init__()
        self._requires_x   = False
        self._requires_u   = False
        self._requires_g   = False
        self._requires_hvp = False
        self._requires_H   = False
        self._allows_exact = False

    def header_str(self):
        pass

    def update_str(self):
        pass

    def update_composition(self, *args, **kwargs):
        pass

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=False):
        """ gradient_eval = True triggers special behaviors for the inner-loop
            calls of the functionals, where we're interested in gradients more
            than exact evaluation of F.
        """
        pass

    def exact_eval(self, *args, **kwargs):
        pass

    def reset(self, *args, **kwargs):
        pass

class FunctionalSum(Functional):
    """ Sum of functionals """
    def __init__(self, funs, weights=None, *args, **kwargs):
        super(FunctionalSum, self).__init__(*args, **kwargs)
        self.funs = funs
        if weights is None:
            weights = [1]*len(self.funs)
        self.weights = weights
        assert len(funs) == len(weights)
        # Add todo: check for weight len
        self._requires_x = any([f._requires_x for f in self.funs])
        self._requires_u = any([f._requires_u for f in self.funs])
        self._requires_g = any([f._requires_g for f in self.funs])
        self._requires_hvp = any([f._requires_hvp for f in self.funs])
        self._requires_H = any([f._requires_H for f in self.funs])
        self._allows_exact = True # will pass on decision to submodules
        self._F_œÅ = None

    def header_str(self):
        s = '{:>8}'.format('F(œÅ)= ')
        for i,f in enumerate(self.funs):
            s += '   + ' if i > 0 else '     '
            s += f.header_str().replace('F(œÅ)=', '')
        return s

    def update_str(self):
        s = f'{self._F_œÅ:8.2f}'
        for f in self.funs:
            s += f.update_str()
        return s

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        total=0
        for i,f in enumerate(self.funs):
            total += self.weights[i]*f(x,u,g,hvp,H,gradient_eval)
        self._F_œÅ = total
        return total

    def exact_eval(self, *args, **kwargs):
        total=0
        for i,f in enumerate(self.funs):
            if f._allows_exact:
                total += self.weights[i]*f.exact_eval(*args, **kwargs)
        return total

    def reset(self, *args, **kwargs):
        for i,f in enumerate(self.funs):
            f.reset(*args, **kwargs)

class PotentialFunctional(Functional):
    """ A functional of the form:
            F(œÅ) = ‚à´V(x)dœÅ(x)
        The gradient flow of this functional corresponds to an advection PDE:
            ‚àÇœÅ/‚àÇt = ‚àá¬∑(œÅ ‚àáV)
    """
    def __init__(self, V, *args, **kwargs):
        super(PotentialFunctional, self).__init__(*args, **kwargs)
        self.V = V
        self._requires_g = True
        self._F_œÅ = None
        self._allows_exact = True

    def header_str(self):
        return '{:>10}'.format('F(œÅ)=ùí±(œÅ)')

    def update_str(self):
        return f'{self._F_œÅ:10.2e}'

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        out = self.V(g).mean()  # V(G(X))
        self._F_œÅ = out.item()
        return out

    def exact_eval(self, x, œÅ=None, œÅx=None, **kwargs):
        " Exact evaluation of functional when given the analytic density """
        # manual integration: self.f(torch.exp(œÅ.log_prob(x))).mean()
        # torch integration:
        if œÅx is not None:
            y = self.V(x) * œÅx
        else:
            y = self.V(x) * torch.exp(œÅ.log_prob(x))
        integral = torch.trapz(y.squeeze(), x.squeeze())
        return integral

    def reset(self,  *args, **kwargs):
        self._F_œÅ = None


class InteractionFunctional(Functional):
    """ A functional of the form:
            F(œÅ) = ‚à´W(x-y)dœÅ(x)
        The gradient flow of this functional corresponds to an advection PDE:
            ‚àÇœÅ/‚àÇt = ‚àá¬∑(œÅ ‚àáW * œÅ)
    """
    def __init__(self, W, *args, **kwargs):
        super(InteractionFunctional, self).__init__(*args, **kwargs)
        self.W = W
        self._requires_g = True
        self._F_œÅ = None
        self._allows_exact = True

    def header_str(self):
        return '{:>10}'.format('F(œÅ)=ùí≤(œÅ)')

    def update_str(self):
        return f'{self._F_œÅ:10.2e}'

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        # Split samples into two batches
        idx = torch.randperm(x.shape[0])
        idx1,idx2 = idx[:int(x.shape[0]/2)], idx[int(x.shape[0]/2):]

        ## Method 1: single avergage - cruder approximation, faster
        # assert x.shape[0] % 2 == 0, 'single expectation method works for even-sixed samples only'
        # out = self.W(g[idx1,:] - g[idx2,:]).mean() * 0.5

        ## Method 2: iterated averages - works even for odd sample size
        G = (g[idx1,:].unsqueeze(1) - g[idx2,:])   # n1 x n2 x d, Gij = xi - xj
        out = self.W(G).squeeze().mean() * 0.5

        self._F_œÅ = out.item()
        return out

    def sample_eval(self, X, œÅ=None, eval_points=100, **kwargs):
        # Method 1: Double Expectation
        idx = torch.randperm(X.shape[0])
        idx1,idx2 = idx[:int(X.shape[0]/2)], idx[int(X.shape[0]/2):]
        G = (X[idx1,:].unsqueeze(1) - X[idx2,:])   # n1 x n2 x d, Gij = xi - xj
        expectation = self.W(G).squeeze().mean()
        return expectation * 0.5


    def exact_eval(self, x, œÅ=None, œÅx=None, **kwargs):
        " Exact evaluation via integration, when given the analytic density """
        xv, yv = torch.meshgrid([x.squeeze(),x.squeeze()])
        if œÅx is None:
            œÅx = torch.exp(œÅ.log_prob(x)).squeeze()
        else:
            œÅx = œÅx.squeeze()

        # Method 1:
        # z = self.W(xv - yv) * œÅx
        # integral = torch.trapz((torch.trapz(z, x.squeeze(), dim=0) * œÅx).squeeze(), x.squeeze())

        # Method 2: (does seem equivalent)
        Z = self.W((xv - yv).unsqueeze(-1)).fill_diagonal_(0) * (torch.outer(œÅx,œÅx))
        # Need to replace inf->0 in diagonal for the functional we're using because it thakes log of difference

        integral = torch.trapz(torch.trapz(Z, x.squeeze(), dim=0).squeeze(), x.squeeze())

        return integral * 0.5

class InternalEnergyFunctional(Functional):
    """ A functional of the form:
            F(œÅ) = ‚à´f(œÅ(x))dx
    """
    def __init__(self, f, œÅ, X=None, multfun = None, cg_iters=50, m_lanczos=10, *args, **kwargs):
        super(InternalEnergyFunctional, self).__init__(*args, **kwargs)
        self._allows_exact = True
        self.f = f
        self.multfun = multfun
        self.œÅ0 = œÅ
        self.X0 = X
        self.m_lanczos = m_lanczos
        self.cg_iters = cg_iters
        self._requires_x = True
        self._requires_hvp = True
        self.bsz, *dims = X.shape
        self.dim = np.prod(dims)
        #self._requires_H = (self.dim <= 1) # Only needed if using Method 1 for logdet
        self.logdet_composition = torch.zeros(self.bsz)
        if hasattr(self.œÅ0, 'reset'): self.œÅ0.reset()
        if X is not None:
            self.logprob_X0 = self.œÅ0.log_prob(X.detach())
        else:
            self.logprob_X0 = None
        self._F_œÅ = None


    def reset(self, X=None, *args, **kwargs):
        if X is not None: #new sample
            print('here')
            self.X0 = X
            self.bsz, *dims = X.shape
        self.logdet_composition = torch.zeros(self.bsz)
        if hasattr(self.œÅ0, 'reset'): self.œÅ0.reset()
        self.logprob_X0 = self.œÅ0.log_prob(self.X0.detach())
        self._F_œÅ = None

    def header_str(self):
        return '{:>10}'.format('F(œÅ)=‚Ñ±(œÅ)')

    def update_str(self):
        return f'{self._F_œÅ:10.2f}'

    def update_composition(self, logdet):
        self.logdet_composition += logdet.detach()

    #def eval_update(self, x=None, u=None, g=None, hvp=None, H=None):

    # def exact_eval(self, x, œÅ=None, œÅx=None, **kwargs):
    #     " Exact evaluation of functional when given the analytic density """
    #     # manual integration: self.f(torch.exp(œÅ.log_prob(x))).mean()
    #     # torch integration:
    #     if œÅx is not None:
    #         y = self.f(œÅx)
    #     else:
    #         y = self.f(torch.exp(œÅ.log_prob(x)))
    #     integral = torch.trapz(y.squeeze(), x.squeeze())
    #     return integral

    def exact_eval(self, x=None, œÅ=None, œÅx=None, **kwargs):
        """ Exact evaluation of functional via integral ‚à´œÅ(x)log œÅ(x)dx
            - requires having analytic density œÅ or evaluated at grid œÅx
            - only feasible in 1D or 2D

            Takes either x,œÅ or x,œÅx as args. If given x, is assumed to be a
            uniform grid.
        """
        #pdb.set_trace()
        assert not (œÅ is None and œÅx is None)
        use_deltas=True
        with torch.no_grad():
            if œÅx is not None:
                fx = self.f(œÅx)
            else:
                if x is None:
                    x, npoints = meshgrid_from(œÅ=œÅ)

                if x.ndim == 1:
                    x = x.view(-1, 1)
                if hasattr(œÅ, 'log_prob'):
                    fx = self.f(torch.exp(œÅ.log_prob(x)))
                else: # last ditch attempted - use directly as callable
                    fx = self.f(torch.exp(œÅ(x)))

            fx = fx.view(x.shape)
            assert x.ndim == 2, 'x should be n x d'
            assert fx.ndim == 2, 'fx should be n x d'
            ## Filter out nans - these are zero mass points, don't add to integral
            _x  = x[~torch.isinf(fx).squeeze(),:]
            _fx = fx[~torch.isinf(fx).squeeze(),:]

            def get_delta(u):
                a, b = torch.unique(u)[:2]
                return b-a
            dxs = [get_delta(_x[:,i]) for i in range(x.shape[1])]


            if x.shape[1] == 1: #1D
                if use_deltas:
                    integral = torch.trapz(_fx.squeeze(), dx=dxs[0])
                else:
                    pdb.set_trace()
                    integral = torch.trapz(_fx.squeeze(), x.squeeze())
            elif x.shape[1] == 2: #2D
                if use_deltas:
                    integral = torch.trapz(torch.trapz(_fx.reshape(npoints, npoints), dx=dxs[0], dim=0), dx=dxs[1], dim=0)
                else:
                    pdb.set_trace(header='Not implemented yet')
            else:
                raise NotImplementedError()
        return integral

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        bsz, *dims = x.shape
        dim = np.prod(dims)
        update_cache = not gradient_eval
        ### Use direct objective for the theta-independent multiplier
        with torch.no_grad():
            #pdb.set_trace()
            if self.dim > 1:
                # vector must be normalized for Qr decomposition in lanczos_tridiagonalization
                v1 = torch.nn.functional.normalize(sample_rademacher(bsz, dim)).to(x)
                #logdet = unbiased_logdet(hvp, v, p=0.1, n_exact_terms=4)
                logdet = stochastic_lanczos_quadrature(hvp, v1, m=max(2,min(self.m_lanczos,dim)))
                #logdet = stochastic_logdet_gradient_estimator(hvp, v, self.cg_iters, rtol=0.0, atol=1e-6)
            else:
                # In 1D, H is a scalar, so lodget(H) = log abs(H), no need to estimate
                # Method 1: via actual hessian
                #logdet = torch.log(H.reshape(bsz,))
                # Method 2: via hvp
                logdet = torch.log(hvp(torch.ones(bsz)).squeeze())
                if torch.isnan(logdet).any():
                    if not OVERRIDE_NANS_WARNING: pdb.set_trace(header='nans in logdet')

            assert self.logprob_X0.shape == logdet.shape, print(self.logprob_X0.shape, logdet.shape)
            logratio = self.logprob_X0 - self.logdet_composition - logdet
            ratio = torch.exp(logratio)

        if self.multfun is None:
            # We use the .sum() trick: d/dx_i f(x_i) = d/d_xi sum_i(f(x))
            _ratio = ratio.detach().clone().requires_grad_(True)
            f_p = torch.autograd.grad(self.f(_ratio).sum(), _ratio, create_graph=True)[0]
            multiplier = f_p - self.f(ratio)/ratio
        else:
            multiplier = self.multfun(logratio)


        if torch.isnan(multiplier).any():
            print('Nans in multiplier')
            if not OVERRIDE_NANS_WARNING: pdb.set_trace()

        ### Now get gradient with conjugate_gradient method on surrogate objective
        if self.dim > 1:
            v = sample_rademacher(bsz, dim).to(x)
            logdet_estim = stochastic_logdet_gradient_estimator(hvp, v, self.cg_iters, rtol=0.0, atol=1e-3)
        else:
            #logdet_estim = torch.log(H.reshape(bsz,1))
            logdet_estim = torch.log(hvp(torch.ones(bsz)).squeeze())

        surrogate_loss = -multiplier*logdet_estim
        F_œÅ = surrogate_loss.mean()

        # #ratio = self.density/torch.exp(self.logdet_composition + logdet)
        # logratio = self.logprob_X0 - self.logdet_composition - logdet
        # ratio = torch.exp(logratio)
        # F_œÅ = (self.f(ratio)/ratio).mean()

        if torch.isnan(F_œÅ).any():
            print('Nans in F_œÅ: ')
            if torch.isnan(logdet_estim).any(): print('Nans in logdet estim')
            if torch.isnan(logratio).any(): print('Nans in multiplier')
            if torch.isnan(multiplier).any(): print('Nans in multiplier')
            if not OVERRIDE_NANS_WARNING: pdb.set_trace()

        self._F_œÅ = F_œÅ.item()

        if update_cache:
            print(f'Updating cache with: {self.logprob_X0[0].item():8.2e}  '
                  f'{self.logdet_composition[0].item():8.2e}  '
                  f'{logdet[0].item():8.2e}  '
                  f'{ratio[0].item():8.2e}  ')
            self.update_composition(logdet)

        return F_œÅ


class EntropyFunctional(Functional):
    """ A functional of the form:
            F(œÅ) = ‚à´œÅ(x)log œÅ(x)dx
        This is a particular case of the internal energy functional with t(y) = y log y
    """
    def __init__(self, œÅ0=None, dim=1, cg_iters = 10, m_lanczos=2,
                value_eval_method='logdet', *args, **kwargs):
        super(EntropyFunctional, self).__init__(*args, **kwargs)
        self.cg_iters = cg_iters
        self.m_lanczos = m_lanczos
        self._requires_x = True
        self._requires_hvp = True
        #self.compute_exact = compute_exact
        self.value_eval_method = value_eval_method
        #assert not (compute_exact and œÅ0 is None), "exact computation of entropy requires œÅ0 be provided"
        self.œÅ0 = œÅ0
        self._allows_exact = ( œÅ0 is not None ) and (dim <= 2)
        assert not (value_eval_method == 'logdet' and œÅ0 is None)
        self._F_œÅ = None      # Value of true objective
        self._F_hat_œÅ = None  # Value of surrogate objective
        if self._allows_exact and value_eval_method == 'exact':
            #domain = œÅ0.get_domain() if hasattr(œÅ0, 'get_domain') else (-2,2a)
            #x = meshgrid_from(œÅ=œÅ0)
            self._F_œÅ = self.exact_eval(œÅ=œÅ0)
        else:
            ### FIXME: is there anything we can do here?
            self._F_œÅ = 0


    def reset(self, *args, **kwargs):
        self._F_œÅ = None      # Value of true objective
        if self._allows_exact and self.value_eval_method == 'exact':
            #domain = œÅ0.get_domain() if hasattr(œÅ0, 'get_domain') else (-2,2a)
            #x = meshgrid_from(œÅ=œÅ0)
            self._F_œÅ = self.exact_eval(œÅ=self.œÅ0)
        else:
            ### FIXME: is there anything we can do here?
            self._F_œÅ = 0
        self._F_hat_œÅ = None  # Value of surrogate objective

    def header_str(self):
        return '{:>12}'.format('F(œÅ)=‚Ñ±*(œÅ)')

    def update_str(self):
        return f'{self._F_hat_œÅ:12.2e}'

    def exact_eval(self, x=None, œÅ=None, œÅx=None, **kwargs):
        """ Exact evaluation of functional via integral ‚à´œÅ(x)log œÅ(x)dx
            - requires having analytic density œÅ or evaluated at grid œÅx
            - only feasible in 1D or 2D

            Takes either x,œÅ or x,œÅx as args. If given x, is assumed to be a
            uniform grid.
        """
        assert not (œÅ is None and œÅx is None)
        use_deltas=True
        with torch.no_grad():
            if œÅx is not None:
                fx = œÅx*torch.log(œÅx)
            else:
                if x is None:
                    x, npoints = meshgrid_from(œÅ=œÅ)
                logp = œÅ.log_prob(x)
                fx = torch.exp(logp) * logp

            def get_delta(u):
                a, b = torch.unique(u)[:2]
                return b-a
            dxs = [get_delta(x[:,i]) for i in range(x.shape[1])]

            if x.shape[1] == 1: #1D
                if use_deltas:
                    integral = torch.trapz(fx.squeeze(), dx=dxs[0])
                else:
                    pdb.set_trace()
                    integral = torch.trapz(fx.squeeze(), x.squeeze())
            elif x.shape[1] == 2: #2D
                if use_deltas:
                    integral = torch.trapz(torch.trapz(fx.reshape(npoints, npoints), dx=dxs[0], dim=0), dx=dxs[1], dim=0)
                else:
                    pdb.set_trace(header='Not implemented yet')
            else:
                raise NotImplementedError()

        return integral


    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        bsz, *dims = x.shape
        dim = np.prod(dims)

        if gradient_eval:
            assert hvp is not None, 'Need to pass hvp when gradient_eval=True'
            #print('here')
            ## For the loss to backprop, we use the surrogate objective
            v = sample_rademacher(bsz, dim).to(x)
            logdet = stochastic_logdet_gradient_estimator(hvp, v, self.cg_iters, rtol=0.0, atol=1e-3)
            value = -logdet.mean()
            #pdb.set_trace()
            #print(value.item())
        elif self.value_eval_method == 'logdet' or not self._allows_exact:
            ## For debugging, we use the unbiased log dest estimater + complete objective
            # F(œÅ_{t+1}) = F(œÅ_t) - E[logdet]
            with torch.no_grad():
                if dim > 1:
                    # vector must be normalized for Qr decomposition in lanczos_tridiagonalization
                    v1 = torch.nn.functional.normalize(sample_rademacher(bsz, dim)).to(x)
                    #logdet = unbiased_logdet(hvp, v, p=0.1, n_exact_terms=4)
                    logdet_exact = stochastic_lanczos_quadrature(hvp, v1, m=max(2,min(self.m_lanczos,dim)))
                    #logdet = stochastic_logdet_gradient_estimator(hvp, v, self.cg_iters, rtol=0.0, atol=1e-6)
                else:
                    # In 1D, H is a scalar, so lodget(H) = log abs(H), no need to estimate
                    # Method 1: via actual hessian
                    #logdet = torch.log(H.reshape(bsz,))
                    # Method 2: via hvp
                    logdet_exact = torch.log(hvp(torch.ones(bsz)).squeeze())
                value = self._F_œÅ - logdet_exact.mean()
                self._F_œÅ = value # Not that we don't update this value in gradient evaluations
        elif self.value_eval_method == 'exact':
            value = self.exact_eval(x, 'œÅt') # FIXME: where do we get œÅt from? maybe self.flow.œÅt?
            # Should we add it as anothar potential arg to Functional forward calls?
        else:
            raise NotImplementedError('Port from InternalEnergyFunctional')

        self._F_hat_œÅ = value

        return value


class FokkerPlanckFunctional(Functional):
    """ Functional corresponding to the Fokker-Planck equation:
            ‚àÇœÅ/‚àÇt = ‚àÜœÅ + ‚àá¬∑(œÅ ‚àáV)
        This PDE arises as a gradient flow of the functional:
            F(œÅ) = ‚Ñ±(œÅ) + ùí±(œÅ) = ‚à´œÅ(x)log œÅ(x)dx + ‚à´V(x)dœÅ(x)
    """
    def __init__(self, V, *args, **kwargs):
        super(FokkerPlanckFunctional, self).__init__(*args, **kwargs)
        self.ùí± = PotentialFunctional(V)
        self.‚Ñ± = EntropyFunctional()
        self._requires_g = True    # needed by ùí±
        self._requires_x = True    # needed by ‚Ñ±
        self._requires_hvp = True  # needed by ‚Ñ±
        self._F_œÅ = None
        self._cV_œÅ = None
        self._cF_œÅ = None

    def header_str(self):
        return '{:>8} {:>8} {:>8}'.format('F(œÅ)=','‚Ñ±(œÅ)','+   ùí±(œÅ)')

    def update_str(self):
        return f'{self._F_œÅ:8.2f} {self._cF_œÅ:8.2f} {self._cV_œÅ:8.2f}'

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        ‚Ñ±_œÅ = self.‚Ñ±(x, u, g, hvp, H)
        ùí±_œÅ = self.ùí±(x, u, g, hvp, H)
        self._cF_œÅ = ‚Ñ±_œÅ.item()
        self._cV_œÅ = ùí±_œÅ.item()
        self._F_œÅ  = self._cF_œÅ + self._cV_œÅ
        #pdb.set_trace()
        return ‚Ñ±_œÅ + ùí±_œÅ

class DiffAdvIntFunctional(Functional):
    """ Functional corresponding to the general diffusion-advection-interaction PDE:
            ‚àÇœÅ/‚àÇt = ‚àá¬∑(œÅ [‚àá(f'(œÅ)) + ‚àáV + (‚àáW)*œÅ] )
        This PDE arises as a gradient flow of the functional:
            F(œÅ) = ‚Ñ±(œÅ) + ùí±(œÅ) + ùí≤(œÅ) = ‚à´f(œÅ(x))dx + ‚à´V(x)dœÅ(x) + ¬Ω‚à´‚à´W(x-y)dœÅ(x)dœÅ(y)
    """
    def __init__(self, f, V, W, *args, **kwargs):
        super(DiffAdvIntFunctional, self).__init__(*args, **kwargs)
        self.‚Ñ± = InternalEnergyFunctional(f)
        self.ùí± = PotentialFunctional(V)
        self.ùí≤ = InteractionFunctional(W)
        self._requires_g = True    # needed by ùí±
        self._requires_x = True    # needed by ‚Ñ±
        self._requires_hvp = True  # needed by ‚Ñ±
        # TODO: What does W need?


    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        ‚Ñ±_œÅ = self.‚Ñ±(x, u, g, hvp, H)
        ùí±_œÅ = self.ùí±(x, u, g, hvp, H)
        ùí≤_œÅ = self.ùí≤(x, u, g, hvp, H)
        out = ‚Ñ±_œÅ + ùí±_œÅ + ùí≤_œÅ
        # TODO MAYBE: use FunctionalSum
        return out


class SinkhornDivergenceFunctional(Functional):
    """ A functional of the form:
            F(œÅ) = SinkornDivergence(œÅ, Œ≤)
        where Œ≤ is a target (fixed) measure.
     """
    def __init__(self, Y, loss='sinkhorn', p=2, blur=.05, debias=True, *args, **kwargs):
        super(SinkhornDivergenceFunctional, self).__init__(*args, **kwargs)
        assert loss in ['sinkhorn', 'energy', 'laplacian', 'gaussian']
        self.p = p
        self.debias = debias
        self.blur = blur
        # TODO: have a switch between SamplesLoss and ImagesLoss once the latter is added to the geomloss repo
        self.loss = SamplesLoss(loss, p=p, blur=blur, debias=debias)
        # blur = (1/e)**p
        ### DECIDE: Can do partial instead to avoid explicitly coping Y
        #self.loss = partial(SamplesLoss(loss, p=p, blur=blur, debias=debias), Y)
        self.Y    = Y
        self._requires_g = True
        self._F_œÅ = None

    def header_str(self):
        return '{:>12}'.format('F(œÅ)=SD(œÅ,Œ≤)')

    def update_str(self):
        return f'{self._F_œÅ:12.2e}'

    def forward(self, x=None, u=None, g=None, hvp=None, H=None, gradient_eval=True):
        bsz, *dims = x.shape
        dim = np.prod(dims)
        sd = self.loss(g.view(g.shape[0], -1), self.Y)
        self._F_œÅ = sd.item()
        return sd
