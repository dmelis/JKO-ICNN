{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import importlib\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage of torch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784]) tensor(-0.0357, grad_fn=<MinBackward1>)\n",
      "torch.Size([100]) tensor(-0.0355, grad_fn=<MinBackward1>)\n",
      "torch.Size([50, 784]) tensor(-0.0357, grad_fn=<MinBackward1>)\n",
      "torch.Size([50, 100]) tensor(0., grad_fn=<MinBackward1>)\n",
      "torch.Size([50]) tensor(-0.0339, grad_fn=<MinBackward1>)\n",
      "torch.Size([25, 784]) tensor(-0.0357, grad_fn=<MinBackward1>)\n",
      "torch.Size([25, 50]) tensor(0., grad_fn=<MinBackward1>)\n",
      "torch.Size([25]) tensor(-0.0333, grad_fn=<MinBackward1>)\n",
      "torch.Size([1, 784]) tensor(-0.0356, grad_fn=<MinBackward1>)\n",
      "torch.Size([1, 25]) tensor(0., grad_fn=<MinBackward1>)\n",
      "torch.Size([1]) tensor(-0.0282, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a fully input convex neural net\n",
    "from src.icnn import FICNN\n",
    "\n",
    "net = FICNN(hidden_dims=[100,50,25])\n",
    "\n",
    "X = torch.randn(128, 28, 28)\n",
    "FX = net(X)\n",
    "\n",
    "\n",
    "for p in net.parameters():\n",
    "    print(p.shape, p.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convexity never violated for 1000 tested pairs.\n"
     ]
    }
   ],
   "source": [
    "import src.utils\n",
    "importlib.reload(src.utils)\n",
    "from src.utils import test_convexity, test_weights\n",
    "\n",
    "\n",
    "test_weights(net)\n",
    "\n",
    "test_convexity(net)\n",
    "\n",
    "X = torch.randn(100, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a partially input convex neural net\n",
    "from src.icnn import PICNN\n",
    "\n",
    "X = torch.randn([128, 30])\n",
    "Y = torch.randn([128, 50])\n",
    "\n",
    "net = PICNN(x_dim = 30, y_dim = 50)\n",
    "\n",
    "FXY = net(X,Y)\n",
    "\n",
    "print(FXY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing with ICNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1313])\n",
      "torch.Size([2000, 24]) torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "# Generate some data: true linear relationship\n",
    "indim = 24\n",
    "outdim = 1\n",
    "n = 2000\n",
    "X = torch.randn(n,indim, requires_grad=True)\n",
    "W = torch.randn(outdim,indim)\n",
    "b = torch.randn(outdim)\n",
    "ε = 1e-8*torch.randn(outdim)\n",
    "Y = torch.relu(torch.matmul(X,W.t()) + b + ε )   # To make target non-negative, otherwise nonline and relu nets wont model it\n",
    "Y = Y.detach()\n",
    "\n",
    "print(b)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositiveWeightClipper(object):\n",
    "    def __init__(self, frequency=1):\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __call__(self, module):\n",
    "        tol = 1e-8\n",
    "        # filter the variables to get the ones you want\n",
    "        if hasattr(module, 'weight_z') and module.weight_z is not None:\n",
    "            w = module.weight_z.data\n",
    "            w = w.clamp(tol,None)\n",
    "            module.weight_z.data = w\n",
    "#         if hasattr(module, 'weight_y'):\n",
    "#             print(module.weight_y.data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.476366996765137\n",
      "Convexity never violated for 1000 tested pairs.\n",
      "1 4.300981044769287\n",
      "2 3.226853609085083\n",
      "3 4.03611946105957\n",
      "4 3.279298782348633\n",
      "5 2.8895609378814697\n",
      "6 3.208811044692993\n",
      "7 2.8981096744537354\n",
      "8 3.266568422317505\n",
      "9 2.2652294635772705\n",
      "10 1.8202370405197144\n",
      "Convexity never violated for 1000 tested pairs.\n",
      "11 1.5617696046829224\n",
      "12 1.915873646736145\n",
      "13 2.138279676437378\n",
      "14 1.8523263931274414\n",
      "15 1.9442164897918701\n",
      "0 1.728805422782898\n",
      "Convexity violated! Curvature: tensor([[-4.7684e-07]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-d13249b9cf56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtest_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtest_convexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/OTICNN/src/utils.py\u001b[0m in \u001b[0;36mtest_convexity\u001b[0;34m(f, niter)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurvature\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Convexity violated! Curvature: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurvature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Convexity never violated for {} tested pairs.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate some data: true linear relationship\n",
    "import src.icnn\n",
    "importlib.reload(src.icnn)\n",
    "importlib.reload(src.utils)\n",
    "from src.icnn import FICNN\n",
    "from src.utils import test_convexity, test_weights\n",
    "from torch.nn import Linear, ReLU\n",
    "\n",
    "\n",
    "# DEBUG test:\n",
    "#f = torch.nn.Sequential(Linear(indim,10),ReLU(), Linear(10,5), ReLU(), Linear(5,outdim), ReLU())\n",
    "f = FICNN(input_dim=indim, hidden_dims=[10,10,5], output_dim = 1, dropout=0, nonlin='relu')\n",
    "optimizer = torch.optim.Adam(f.parameters(), lr=1e-2) # Seems like faster learning rate messes up things\n",
    "                                                      # probably because of crude clipping\n",
    "clipper = PositiveWeightClipper()\n",
    "\n",
    "\n",
    "# create dataset and dataloaders\n",
    "loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X,Y), batch_size=128)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for it,(x,y) in enumerate(loader):\n",
    "        net.zero_grad()\n",
    "        loss = torch.norm(f(x) - y)**2/(2*y.shape[0])\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        f.apply(clipper)\n",
    "        print(it,loss.item())\n",
    "        if it % 10 == 0:\n",
    "            test_weights(f)\n",
    "            test_convexity(f)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0175, 5.2461],\n",
       "        [0.0000, 0.0000],\n",
       "        [3.1047, 2.3003],\n",
       "        [2.1032, 0.0000],\n",
       "        [0.4061, 0.0000],\n",
       "        [1.2425, 0.0000],\n",
       "        [4.4283, 4.8646],\n",
       "        [0.2079, 0.0000],\n",
       "        [0.5294, 0.0000],\n",
       "        [2.0367, 0.0000]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([f(X),Y],1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param Type    min val    max val\n",
      "bias            -0.12       0.27\n",
      "weight_y        -0.32       0.33\n",
      "bias            -0.08       0.32\n",
      "weight_y        -0.33       0.32\n",
      "weight_z         0.02       0.42\n",
      "bias            -0.08       0.28\n",
      "weight_y        -0.35       0.33\n",
      "weight_z         0.07       0.40\n",
      "bias             0.13       0.13\n",
      "weight_y        -0.32       0.28\n",
      "weight_z         0.12       0.56\n"
     ]
    }
   ],
   "source": [
    "# Inspect all parameters\n",
    "print('{:10} {:>10} {:>10}'.format('Param Type', 'min val', 'max val'))\n",
    "for m in f.modules():\n",
    "    for t in ['bias', 'weight_y', 'weight_z']:\n",
    "        if hasattr(m, t) and getattr(m,t) is not None:\n",
    "            print('{:10} {:10.2f} {:10.2f}'.format(t,getattr(m,t).min().item(), getattr(m,t).max().item()))    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
